\documentclass{article}

\title{CS 452 Kernel 3 Documentation}
\author{
  Avi Itskovich, 20332164
  \and
  Alex Ianus, 20342535
}

\begin{document}

\maketitle

\section{Overview}

This documentation covers the implementation of the Kernel for Assignment 3. This assignment covered AwaitEvent and the ClockServer. The new system calls are: 
\begin{itemize}
  \item int AwaitEvent(int eventid)
  \item int WaitTid(int tid)
  \item int Shutdown()
\end{itemize}
The ClockServer has the following interface:
\begin{itemize}
  \item int Delay(int ticks)
  \item int Time()
  \item int DelayUntil(int ticks)
\end{itemize}
The appropriate client tasks to test this were also implemented.

\section{Operating Instructions}
\begin{enumerate}
  \item load -b 0x00218000 -h 10.15.167.4 \$executable
  \item go
\end{enumerate}

\section{Submitted Files}
Root directory: /u1/aianus/cs452/handin/kernel3/

\subsection{Executable}
\begin{verbatim}
b14f26808ede69d80c9dcc2b4f8490bf  ./kernel3.elf
\end{verbatim}
\subsection{Code}
\begin{verbatim}
7a43181f7908eef779e270832fcd9148  ./include/bits.h
d39148360f7d205cb98a36d0a0c61aad  ./include/bwio.h
65779c1b19758a33bae259538343ca1a  ./include/circular_queue.h
be2b7410d592676f04ff4d48aa5189b7  ./include/event.h
0f441d8f0617a666e04bd48fcae98934  ./include/interrupt.h
b044b4c9d77f1d0dd9b7a2961e45a67a  ./include/kmemory.h
9b78bb4a50534ccd3108a1909d0f93cf  ./include/ksyscalls.h
7a3e309d49993b24f39a167f0f408af2  ./include/libc/dassert.h
e31e1aa5f0cf7747c5ca02d9219265d0  ./include/libc/fine_timer.h
8aa1b9a0f41dfeed6de08a271dba5a96  ./include/libc/heap_priority_queue.h
c94aef973f79346c0f65cb42d3a4f92e  ./include/libc/log.h
5c61838a4125651ebe41601ccd881455  ./include/libc/random.h
ea6da646f6f0125c2c3d7c58f8ec5add  ./include/libc/syscall.h
4f590e6dac2e78c18f7082bb2f3b6b92  ./include/memory.h
f5d65e70556962f5effba160e9bbd0a0  ./include/messaging.h
bf705acd9b43d3159d764776a5c6a47a  ./include/nameserver.h
a4b612d477dfd0e5af41e091ebe485c7  ./include/nameservice.h
081b58b1ad6e401d9648cb88ffb30301  ./include/request.h
18060a9170044e77e2c91364f48054f8  ./include/scheduling.h
3f34ab2a8151fc445ead37814f231fb5  ./include/string.h
ccac3c439d86709412554719f50434cc  ./include/switch.h
9941a141c4fc24ccb7888f1a33f28f1e  ./include/task.h
027d930da4ccbbe691b9a6eaf6488c9d  ./include/ts7200.h
92023eb7f5b51008caee8b80306fde73  ./include/user/clock/clock.h
67d3408c795e016994abc7daccaf2dfb  ./include/user/clock/clock_notifier.h
e9f7e445c32353f3b72917ecf553de05  ./include/user/clock/clock_server.h
c43444432320abfb09777821a38ae8a7  ./include/user/idle.h
a53c63ad1757b25362486bce6ef7c90d  ./include/user/rps/rps.h
570c77afbd19a6a0be4419d136ee9a1f  ./include/user/rps/rpsclient.h
331b47449d891d458b0647da2c2924de  ./include/user/rps/rpsserver.h
862df1e336ef89d83bceadc522085dd1  ./include/user/user.h
94833a9ccce763038303123e95428de8  ./include/waiting.h
c946f75ca4551a445ac3eda46091b7be  ./src/bits.c
25757ddee6d82a16be926075e04b4972  ./src/bwio.c
6d5afd3fb8acbda06a47d1d0472d4c33  ./src/circular_queue.c
d6332831f9ea3b52c905ddc59952ed22  ./src/event.c
2e631d9cf59f75a3c44e100db0673a9c  ./src/interrupt.c
82f09f2e13a953ad6ef1a01bba21a6be  ./src/kernel.c
0b13bcc76733e2c2d97c2c66be1de5b7  ./src/ksyscalls.c
df0e33ec5df1306650e909e7463c3834  ./src/libc/fine_timer.c
6f89e16a8c7746ec3c059cedf58171ee  ./src/libc/heap_priority_queue.c
5fc41e8a4b051578f782dc89eefa8d54  ./src/libc/log_arm.c
686d7bdcad6b2a66f66e983565ffc8df  ./src/libc/log_x86.c
18c9fceb4c8379e97a1536c5f76b59df  ./src/libc/memory.c
428d29b72bb432d11e63812e3f19dedd  ./src/libc/random.c
8e2a9827fd752cd6e32094c2596f7642  ./src/libc/syscall.c
77697972cdac93a02951f8476ce29379  ./src/messaging.c
753fb968cab4c991df90b7bee8d9f537  ./src/nameserver.c
af3d35b80511f9dd9565d41583f00315  ./src/nameservice.c
24c1f75f5c74bcb0502e86129fdfeee6  ./src/orex.ld
cbc4b3cdae62ea6a211c5a0f834e646c  ./src/scheduling.c
2e6e7d91fdc2902f90a16534f18837d1  ./src/string.c
3d07be1b7e34d5dddb0f2981f80eeed9  ./src/switch.c
d78fe35273c566dda31577699c95b6ab  ./src/task.c
6f028faf32aa5991d9ceb153812a7f35  ./src/test/circular_queue_tests.c
60717386c53f8d45228bb945cf8242fb  ./src/test/ksyscalls_tests.c
4eb9af7de10d0189f0c57e83bcdd3e83  ./src/test/memory_tests.c
7a1f27a3517346480a2a24254e7422d5  ./src/test/messaging_tests.c
535c57bf8e16fea12c0e1fb2e983ee4b  ./src/test/priority_queue_tests.c
c3b46c3acfa81d47b2e01f9e1a223543  ./src/test/scheduling_tests.c
ea278c94bfbbf69c9b7c5eb865532005  ./src/user/clock/clock_notifier.c
a055c29d271770794159781fbf86bbad  ./src/user/clock/clock_server.c
d1996992d6f04747c72547b4eea54dfe  ./src/user/idle.c
1e3f84b77a97255d6251ddf262835725  ./src/user/kernel1.c
b23bda2d5f268ed1a85959e266882360  ./src/user/kernel2.c
d8e09c1a0766a1ee4d3f8c6b9c8dc7f8  ./src/user/kernel3.c
46a424731361d6176718c8b7c737bc55  ./src/user/rps/rpsclient.c
8b5ae02bd24de4355a0f7c5fb17e87dc  ./src/user/rps/rpsserver.c
fa6b70558103fa109c508faa51d5071e  ./src/user/timings.c
95197606aa508ee5043c16021c0f6234  ./src/user/user.c
f90b3c33dead7ca55d9192d8112cdc4f  ./src/waiting.c
\end{verbatim}
\section{Kernel Description}

\subsection{Interrupts}

Our interrupt system can be divided into two parts: handling interrupts as they occur and managing them (enable/disable) and bubbling up events to the system.

\subsubsection{Interrupt Handling}

Interrupt handling is all done through a simple set of primitives available in "interrupt.c". These primitives allow you to enable, disable, generate and clear a specific interrupt. Interrupts are defined in an enum and as we have more interesting interrupts we add them to the enum. User programs never deal with "interrupts", the simply enable events and the event system knows which interrupts to enable and does so appropriately.

The interrupt system also has a process\_interrupt function which deals with interpreting one interrupt. Currently, the interrupt system has no special logic about which interrupt to handle first. Instaed we just grab the first set interrupt that we find. This is bad, as it could lead to some interrupts starving others out. However, we will improve this once we have more interrupts to care about (we only care about 1 now). process\_interrupt looks at the current turned on interrupts and once it has found one, converts it into an event, grabs the volatile data and clears the interrupt. The volatile data and event id are passed through to the event system.

To being with all interrupts are turned off in the kernel, as notifiers start-up they will enable the interrupts they care about by enabling the appropriate events. This ensures we don't get flooded by interrupts we don't care about.

In our kernel we chose to use the simple AwaitEvent interface, because we currently only need to return 32 bits of volatile data. In the future we may modify this if more data needs to be returned and we can't find a workaround to return it in 32bits.

\subsubsection{Event Management}

User programs work at the event level. We expose an interface to enable events and wait for them to occur. We currently do not support disabling events. For this assignment, we've kept the event system quite simple. Only one task may wait on any event at one time. Trying to wait on an event that someone else is waiting on produces an error. Due to the single waiter nature of our system internally we use a single array of size NUM\_EVENTS to hold the waiters for each event. If an event occurs while no tasks are waiting for it we log an error to the screen. However, this should not occur as the time between interrupts is currently around 10ms. However, in that time our high priority notifier and high priority server should both have the chance to run as they will be scheduled one after the other. If we see this becoming a problem, we can add a queue to hold missed events which will then be given to tasks on calling AwaitEvent. So far, this has not been an issue.

When the kernel starts up all events are turned off. When notifiers come online, they turn on the events they require and listen for them. Events are exposed to the user through a public enum that lists all events. To add an event you have to add to this enum. This is the simplest implementation that works, and since we have a finite set of peripherals it is good enough.

\subsection{Context Switch}

For this assignment we re-wrote a majority of our context switch code for simplification. Although, our old code was perfectly useful for hardware interrupts, we wanted to speed up our context switch and make it easier to read.

\subsection{Hardware Interrupt}

Our Hardware Interrupt context switch is implemented through a wrapping mechanism. We introduce a new entry point, irq\_enter, which saves the scratch registers on the user's stack before jumping (bl) into our regular mechanism of entering the kernel (kernel\_enter). By using bl to jump into kernel\_enter, we modify the link register so when kernel\_exit completed if you entered kernel\_enter through the irq handler, you come back to it so we can unwrap the scratch registers.

\subsubsection{irq\_enter:}
\begin{enumerate}
    \item Change to System Mode.
    \item Save scratch registers onto user stack (r0, r1, r2, r3, ip).
    \item Change to IRQ Mode.
    \item Grab LR/SPSR.
    \item Change to System Mode. 
    \item Save LR/SPSR onto user stack.
    \item Change to Supervisor Mode.
    \item Insert special value (0) into r0 to represent irq interrupt.
    \item Change our supervisor\_spsr to IRQ Mode.
    \item Jump into Kernel Enter.
\end{enumerate}

\subsubsection{irq\_exit:}
\begin{enumerate}
    \item Switch to System Mode.
    \item Unload LR/SPSR.
    \item Switch to Interrupt Mode.
    \item Install LR/SPSR.
    \item Switch to System Mode.
    \item Reload Scratch Registers (r0, r1, r2, r3, ip)
    \item Switch to IRQ Mode
    \item Jump back into program and install correct cpsr.
\end{enumerate}

Note that irq\_exit does not exist as a label, and is simply a convention to refer to the lines after "bl kernel\_enter".

\subsection{Software Interrupt}

During the implementation of this assignment we decided to overhaul our basic context switch as well. Previously, we stored a lot of data in the Task Descriptor. We have now moved almost all of this data into the user stack. The only information we maintain in the task descriptor is the stack pointer itself. Our new kernel\_enter and kernel\_exit function as follows:

\subsubsection{kernel\_exit:}
\begin{enumerate}
    \item Save Kernel State (including r0 since it contains our task descriptor memory location).
    \item Load the SP for task from it's task descriptor.
    \item Change to System Mode. 
    \item Install Stack Pointer.
    \item Unload User Registers + PC/SPSR from User Stack.
    \item Change to Supervisor Mode.
    \item Install SPSR
    \item Jump to user PC and install correct cpsr.
\end{enumerate}

\subsubsection{kernel\_enter:}
\begin{enumerate}
   \item Save SPSR/Link Register into Common Registers.
   \item Change to System State.
   \item Save User State + SPSR/Link Register on Stack.
   \item Grab User SP in Common Register.
   \item Change to Supervisor State.
   \item Backup Request in Scratch Register.
   \item Load Kernel State.
   \item Store User's SP into Task Descriptor (memory address is in r0).
   \item Install the backed up Request in r0.
   \item Jump into the Kernel.
\end{enumerate}

\subsubsection{Idle Task}

The idle task is implemented as simple for(;;) loop. It has the lowest priority, so it only runs when there's nothing else to do.

\subsection{Clock Server}

\subsubsection{Server}
The clock server, running at priority HIGHEST, is implemented as an infinite message handling loop. It is important that the clock server run at a high priority because we could have other important tasks blocked on a delay and we want them to be unblocked ASAP when their deadline is reached. Additionally, clock server is unlikely to cause any starvation issues since it is almost always blocked waiting for a message.

On a tick message, the internal time of the clock server is incremented by one. Furthermore, any tasks on top of the priority queue whose deadline has passed are unblocked with a Reply.

On a delay message, the sending task is placed in the priority queue with priority equal to the deadline which is equal to the current time + the specified delay.

On a delay\_until message, the deadline is compared to the current time and if it has not yet passed the task is placed on the priority queue with priority equal to the deadline. If the deadline has already passed, the task is not placed on the queue but is immediately unblocked with a Reply.

On a time message, a reply is immediately issued with the current time in ticks.

The priority queue is implemented as a heap so that the most frequent operation, checking the deadline of the head of the queue, is done in very fast constant time. Furthermore, popping and pushing tasks from/onto the queue is also very fast, running in O(logn) time. In addition, the heap is implemented as an array, which should give good cache performance as all the elements are in contiguous memory.

\subsubsection{Notifier}

The clock notifier is implemented as simply as possible. It first looks up the ClockServer. Although it is spawned by the ClockServer, and could simply use it's ParentTid for this. We preferred to make our code more bulletproof and leave out this assumption. After it has found the ClockServer, we enable the TIMER\_3\_EVENT we listen to and setup the timer with the appropriate values. The notifier then listens to ticks and sends them to the clock server as they occur. The notifier is run at the highest priority (REALTIME) in our system because we want it to respond as quickly as possible.

\subsection{Output and Analysis}
\subsubsection{Client task output}
\begin{verbatim}
Delay number 1 completed for task 4 having delay interval 10
Delay number 2 completed for task 4 having delay interval 10
Delay number 1 completed for task 5 having delay interval 23
Delay number 3 completed for task 4 having delay interval 10
Delay number 1 completed for task 6 having delay interval 33
Delay number 4 completed for task 4 having delay interval 10
Delay number 2 completed for task 5 having delay interval 23
Delay number 5 completed for task 4 having delay interval 10
Delay number 6 completed for task 4 having delay interval 10
Delay number 2 completed for task 6 having delay interval 33
Delay number 3 completed for task 5 having delay interval 23
Delay number 1 completed for task 7 having delay interval 71
Delay number 7 completed for task 4 having delay interval 10
Delay number 8 completed for task 4 having delay interval 10
Delay number 4 completed for task 5 having delay interval 23
Delay number 9 completed for task 4 having delay interval 10
Delay number 3 completed for task 6 having delay interval 33
Delay number 10 completed for task 4 having delay interval 10
Delay number 5 completed for task 5 having delay interval 23
Delay number 11 completed for task 4 having delay interval 10
Delay number 12 completed for task 4 having delay interval 10
Delay number 4 completed for task 6 having delay interval 33
Delay number 13 completed for task 4 having delay interval 10
Delay number 6 completed for task 5 having delay interval 23
Delay number 2 completed for task 7 having delay interval 71
Delay number 14 completed for task 4 having delay interval 10
Delay number 15 completed for task 4 having delay interval 10
Delay number 7 completed for task 5 having delay interval 23
Delay number 5 completed for task 6 having delay interval 33
Delay number 16 completed for task 4 having delay interval 10
Delay number 17 completed for task 4 having delay interval 10
Delay number 8 completed for task 5 having delay interval 23
Delay number 18 completed for task 4 having delay interval 10
Delay number 6 completed for task 6 having delay interval 33
Delay number 19 completed for task 4 having delay interval 10
Delay number 9 completed for task 5 having delay interval 23
Delay number 3 completed for task 7 having delay interval 71
Delay number 20 completed for task 4 having delay interval 10
\end{verbatim}

If we map each line to the delay number * the delay interval we see a sequence that is almost strictly increasing, with a few exceptions (such as Delay \#1 for task 7 with interval 71 occurring before Delay \#7 for task 4 with interval 10).  We speculate this drift is caused by the time it takes the task to print its status message after each delay. This is a known pitfall in using Delay repeatedly instead of using DelayUntil which would allow us to measure and bound the drift. 

\subsubsection{Timing stats}
\begin{verbatim}
Task runtime info:
Task 0 ran for 255us or 0 percent of the time
Task 1 ran for 12317us or 0 percent of the time
Task 2 ran for 2112405us or 90 percent of the time
Task 3 ran for 1775us or 0 percent of the time
Task 4 ran for 103355us or 4 percent of the time
Task 5 ran for 46086us or 1 percent of the time
Task 6 ran for 30726us or 1 percent of the time
Task 7 ran for 15846us or 0 percent of the time
Task 8 ran for 1142us or 0 percent of the time
\end{verbatim}

Here we can see the breakdown of how much CPU time is spent executing each task. As expected, the idle task (task 2), is taking up the majority of the CPU since it is executing whenever the other tasks are blocked on a Delay call. Also as expected, the more times a client needs to wake up and print, the more CPU time is spent servicing it.

\subsection{WaitTid}

We implemented WaitTid to make termination simpler in our user programs. Although we could have done something like a Send back to our parent task once we terminate, we decided against because it is more prone to programmer error. Instead we use the Exit call which all programs do anyways to notify tasks that are waiting on it to exit that it is complete. Our "first" program now calls WaitTid on each of it's child tids that it wants to terminate and once those have completed calls shutdown to exit the kernel.

WaitTid is implemented through a simple linked list of task ptrs. We re-use the next ptr that we had originally used in the scheduler for the waiter list. The amount of extra space required to implement WaitTid is 4 bytes * MAX\_TASKS, since we only store the head of the waiter list. Waiting on a task is an O(1) operation (adding to the head of a linked list). Unblocking tasks is at most O(n), but we are already unblocking send blocked tasks which also costs O(n) so it is okay for us to take on the extra hit here.

\subsection{Shutdown}

Shutdown is implemented simply. Once shutdown is called, instead of scheduling the next task we simply exit the FOR(;;) loop and thereby terminate the system going back to Redboot.

\subsection{Addendum}

During our work we noticed we would constantly leave out calls to Exit at the end user tasks. To fix this, we augmented our tasks to automatically call exit when they terminate. We did this by setting the default lr they unload when they start to be pointing to the Exit system call.

\subsection{Known Bugs and Limitations}
\begin{itemize}
    \item An AwaitEvent can only return 32 bits of data.
    \item Only one task may be listening to an event at a time.
    \item We can miss events if our notifier/server is too slow (see event section for argument to why this should not happen).
\end{itemize}

\end{document}
